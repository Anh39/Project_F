[
    {
        "model_name": "gemma2b",
        "sample_amount": 2,
        "base_f1": 0.3829787234042553,
        "lora_f1": 0.3404255319148936
    },
    {
        "seed": 644779,
        "category": "machine_learning",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.7768371105194092,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.1513671875,
        "base_inference_memory": 146.4736328125,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 0.8817706108093262,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3156.8544921875,
        "lora_inference_memory": 146.4736328125
    },
    {
        "seed": 340650,
        "category": "high_school_chemistry",
        "base_predict": "C",
        "base_answer": "A",
        "base_time": 1.051365613937378,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3148.2177734375,
        "base_inference_memory": 156.5400390625,
        "lora_predict": "C",
        "lora_answer": "A",
        "lora_time": 1.1209683418273926,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3166.5224609375,
        "lora_inference_memory": 156.1416015625
    },
    {
        "seed": 605999,
        "category": "high_school_computer_science",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 0.9214940071105957,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3208.29833984375,
        "base_inference_memory": 216.62060546875,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.0258774757385254,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3227.24365234375,
        "lora_inference_memory": 216.86279296875
    },
    {
        "seed": 918503,
        "category": "college_mathematics",
        "base_predict": "C",
        "base_answer": "D",
        "base_time": 0.995781421661377,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3305.48095703125,
        "base_inference_memory": 313.80322265625,
        "lora_predict": "C",
        "lora_answer": "D",
        "lora_time": 1.1472179889678955,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3324.18408203125,
        "lora_inference_memory": 313.80322265625
    },
    {
        "seed": 542291,
        "category": "high_school_european_history",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.4947271347045898,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3609.986328125,
        "base_inference_memory": 618.30859375,
        "lora_predict": "B",
        "lora_answer": "A",
        "lora_time": 1.6246495246887207,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3628.072265625,
        "lora_inference_memory": 617.69140625
    },
    {
        "seed": 991034,
        "category": "electrical_engineering",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 1.165287971496582,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3228.09765625,
        "base_inference_memory": 236.419921875,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 0.6962149143218994,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3153.7958984375,
        "lora_inference_memory": 143.4150390625
    },
    {
        "seed": 380010,
        "category": "high_school_statistics",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 1.9969446659088135,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3581.0126953125,
        "base_inference_memory": 589.3349609375,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 0.8900008201599121,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3327.37158203125,
        "lora_inference_memory": 316.99072265625
    },
    {
        "seed": 859272,
        "category": "global_facts",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.596290111541748,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3148.2177734375,
        "base_inference_memory": 156.5400390625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 0.7199616432189941,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3166.5224609375,
        "lora_inference_memory": 156.1416015625
    },
    {
        "seed": 626695,
        "category": "business_ethics",
        "base_predict": "D",
        "base_answer": "C",
        "base_time": 0.43783092498779297,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.0244140625,
        "base_inference_memory": 145.3466796875,
        "lora_predict": "D",
        "lora_answer": "C",
        "lora_time": 0.5401549339294434,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3155.7275390625,
        "lora_inference_memory": 145.3466796875
    },
    {
        "seed": 82337,
        "category": "professional_medicine",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 0.6069302558898926,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3279.24658203125,
        "base_inference_memory": 287.56884765625,
        "lora_predict": "C",
        "lora_answer": "D",
        "lora_time": 0.6958801746368408,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3297.62158203125,
        "lora_inference_memory": 287.24072265625
    },
    {
        "seed": 903605,
        "category": "econometrics",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.42862534523010254,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.55712890625,
        "base_inference_memory": 151.87939453125,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 1.0223941802978516,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3161.99267578125,
        "lora_inference_memory": 151.61181640625
    },
    {
        "seed": 778101,
        "category": "business_ethics",
        "base_predict": "D",
        "base_answer": "A",
        "base_time": 0.4676334857940674,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3199.66552734375,
        "base_inference_memory": 207.98779296875,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 0.5817193984985352,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3217.82177734375,
        "lora_inference_memory": 207.44091796875
    },
    {
        "seed": 348837,
        "category": "philosophy",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 0.42731189727783203,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.28515625,
        "base_inference_memory": 152.607421875,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.8768634796142578,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.3818359375,
        "lora_inference_memory": 153.0009765625
    },
    {
        "seed": 998550,
        "category": "public_relations",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 1.319237470626831,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.47314453125,
        "base_inference_memory": 146.79541015625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 1.195387840270996,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.17626953125,
        "lora_inference_memory": 146.79541015625
    },
    {
        "seed": 103389,
        "category": "us_foreign_policy",
        "base_predict": "C",
        "base_answer": "B",
        "base_time": 1.5290162563323975,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.79541015625,
        "base_inference_memory": 147.11767578125,
        "lora_predict": "C",
        "lora_answer": "B",
        "lora_time": 1.1410021781921387,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.49853515625,
        "lora_inference_memory": 147.11767578125
    },
    {
        "seed": 629354,
        "category": "sociology",
        "base_predict": "B",
        "base_answer": "A",
        "base_time": 1.6539747714996338,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3146.1474609375,
        "base_inference_memory": 154.4697265625,
        "lora_predict": "C",
        "lora_answer": "A",
        "lora_time": 1.7400751113891602,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3165.4755859375,
        "lora_inference_memory": 155.0947265625
    },
    {
        "seed": 398261,
        "category": "formal_logic",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.7235796451568604,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3381.4990234375,
        "base_inference_memory": 389.8212890625,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 1.8365445137023926,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3399.6240234375,
        "lora_inference_memory": 389.2431640625
    },
    {
        "seed": 178587,
        "category": "high_school_government_and_politics",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 1.4865989685058594,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3146.0771484375,
        "base_inference_memory": 154.3994140625,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 1.5829353332519531,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3164.4287109375,
        "lora_inference_memory": 154.0478515625
    },
    {
        "seed": 220163,
        "category": "college_medicine",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 1.1165776252746582,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3188.9599609375,
        "base_inference_memory": 197.2822265625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 1.2207698822021484,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3207.3505859375,
        "lora_inference_memory": 196.9697265625
    },
    {
        "seed": 879043,
        "category": "high_school_chemistry",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 1.5356812477111816,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3342.6708984375,
        "base_inference_memory": 350.9931640625,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.6722776889801025,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3361.3740234375,
        "lora_inference_memory": 350.9931640625
    },
    {
        "seed": 255165,
        "category": "high_school_macroeconomics",
        "base_predict": "B",
        "base_answer": "B",
        "base_time": 1.0210545063018799,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3191.7880859375,
        "base_inference_memory": 200.1103515625,
        "lora_predict": "B",
        "lora_answer": "B",
        "lora_time": 1.1077170372009277,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3210.4912109375,
        "lora_inference_memory": 200.1103515625
    },
    {
        "seed": 35558,
        "category": "machine_learning",
        "base_predict": "C",
        "base_answer": "A",
        "base_time": 1.267472267150879,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3152.5693359375,
        "base_inference_memory": 160.8916015625,
        "lora_predict": "C",
        "lora_answer": "A",
        "lora_time": 2.342045783996582,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3171.7568359375,
        "lora_inference_memory": 161.3759765625
    },
    {
        "seed": 937329,
        "category": "computer_security",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 2.196218967437744,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.84814453125,
        "base_inference_memory": 152.17041015625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 2.1426408290863037,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.55126953125,
        "lora_inference_memory": 152.17041015625
    },
    {
        "seed": 412573,
        "category": "astronomy",
        "base_predict": "C",
        "base_answer": "B",
        "base_time": 1.45222806930542,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.95654296875,
        "base_inference_memory": 147.27880859375,
        "lora_predict": "C",
        "lora_answer": "B",
        "lora_time": 1.1627299785614014,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.28466796875,
        "lora_inference_memory": 147.90380859375
    },
    {
        "seed": 537025,
        "category": "public_relations",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.4471683502197266,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3167.4833984375,
        "base_inference_memory": 175.8056640625,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 1.3989794254302979,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3185.3662109375,
        "lora_inference_memory": 174.9853515625
    },
    {
        "seed": 200581,
        "category": "philosophy",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.4410209655761719,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3133.80615234375,
        "base_inference_memory": 142.12841796875,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 1.3543272018432617,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3152.50927734375,
        "lora_inference_memory": 142.12841796875
    },
    {
        "seed": 151050,
        "category": "logical_fallacies",
        "base_predict": "C",
        "base_answer": "D",
        "base_time": 1.2820143699645996,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.11767578125,
        "base_inference_memory": 147.43994140625,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 1.4477694034576416,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.82080078125,
        "lora_inference_memory": 147.43994140625
    },
    {
        "seed": 165928,
        "category": "clinical_knowledge",
        "base_predict": "B",
        "base_answer": "D",
        "base_time": 1.2419488430023193,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.63427734375,
        "base_inference_memory": 146.95654296875,
        "lora_predict": "B",
        "lora_answer": "D",
        "lora_time": 1.0419507026672363,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.33740234375,
        "lora_inference_memory": 146.95654296875
    },
    {
        "seed": 786361,
        "category": "business_ethics",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 1.1904239654541016,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.27880859375,
        "base_inference_memory": 147.60107421875,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 1.3883416652679443,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.98193359375,
        "lora_inference_memory": 147.60107421875
    },
    {
        "seed": 476669,
        "category": "high_school_microeconomics",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 1.227907419204712,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3165.4130859375,
        "base_inference_memory": 173.7353515625,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.1806025505065918,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3184.3193359375,
        "lora_inference_memory": 173.9384765625
    },
    {
        "seed": 686494,
        "category": "college_medicine",
        "base_predict": "B",
        "base_answer": "B",
        "base_time": 1.5313880443572998,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3158.9208984375,
        "base_inference_memory": 167.2431640625,
        "lora_predict": "C",
        "lora_answer": "B",
        "lora_time": 2.074646234512329,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3176.9912109375,
        "lora_inference_memory": 166.6103515625
    },
    {
        "seed": 496994,
        "category": "marketing",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 2.3470816612243652,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3259.77099609375,
        "base_inference_memory": 268.09326171875,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 0.8723545074462891,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.46533203125,
        "lora_inference_memory": 148.08447265625
    },
    {
        "seed": 991947,
        "category": "college_biology",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.7752883434295654,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.3125,
        "base_inference_memory": 146.634765625,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 0.8920707702636719,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.015625,
        "lora_inference_memory": 146.634765625
    },
    {
        "seed": 427145,
        "category": "high_school_physics",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 1.20245361328125,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3279.24658203125,
        "base_inference_memory": 287.56884765625,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 1.3060288429260254,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3297.62158203125,
        "lora_inference_memory": 287.24072265625
    },
    {
        "seed": 48909,
        "category": "nutrition",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.8651533126831055,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3179.2255859375,
        "base_inference_memory": 187.5478515625,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 0.9528639316558838,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3197.9287109375,
        "lora_inference_memory": 187.5478515625
    },
    {
        "seed": 643622,
        "category": "high_school_mathematics",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 0.9375696182250977,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3201.87646484375,
        "base_inference_memory": 210.19873046875,
        "lora_predict": "B",
        "lora_answer": "C",
        "lora_time": 1.0344574451446533,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3220.96240234375,
        "lora_inference_memory": 210.58154296875
    },
    {
        "seed": 183467,
        "category": "college_physics",
        "base_predict": "C",
        "base_answer": "A",
        "base_time": 1.0462846755981445,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3272.73095703125,
        "base_inference_memory": 281.05322265625,
        "lora_predict": "C",
        "lora_answer": "A",
        "lora_time": 1.143129825592041,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3291.24658203125,
        "lora_inference_memory": 280.86572265625
    },
    {
        "seed": 294852,
        "category": "high_school_biology",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.9045100212097168,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3163.2724609375,
        "base_inference_memory": 171.5947265625,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 1.0127842426300049,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3182.2255859375,
        "lora_inference_memory": 171.8447265625
    },
    {
        "seed": 214836,
        "category": "medical_genetics",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.8455319404602051,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.26611328125,
        "base_inference_memory": 151.58837890625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 0.9783990383148193,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3161.96923828125,
        "lora_inference_memory": 151.58837890625
    },
    {
        "seed": 575843,
        "category": "business_ethics",
        "base_predict": "D",
        "base_answer": "A",
        "base_time": 0.6786882877349854,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3189.6943359375,
        "base_inference_memory": 198.0166015625,
        "lora_predict": "D",
        "lora_answer": "A",
        "lora_time": 0.7765238285064697,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3208.3974609375,
        "lora_inference_memory": 198.0166015625
    },
    {
        "seed": 392540,
        "category": "high_school_biology",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.719367504119873,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3227.56396484375,
        "base_inference_memory": 235.88623046875,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 0.8116950988769531,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3246.08740234375,
        "lora_inference_memory": 235.70654296875
    },
    {
        "seed": 381433,
        "category": "electrical_engineering",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.5125713348388672,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.34619140625,
        "base_inference_memory": 145.66845703125,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 1.1436882019042969,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3156.04931640625,
        "lora_inference_memory": 145.66845703125
    },
    {
        "seed": 788233,
        "category": "management",
        "base_predict": "B",
        "base_answer": "B",
        "base_time": 1.4493882656097412,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.18505859375,
        "base_inference_memory": 145.50732421875,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 1.089919090270996,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3155.88818359375,
        "lora_inference_memory": 145.50732421875
    },
    {
        "seed": 154657,
        "category": "sociology",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 1.3776230812072754,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3197.59521484375,
        "base_inference_memory": 205.91748046875,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 1.4357342720031738,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3216.77490234375,
        "lora_inference_memory": 206.39404296875
    },
    {
        "seed": 659086,
        "category": "anatomy",
        "base_predict": "D",
        "base_answer": "B",
        "base_time": 0.9447896480560303,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.57568359375,
        "base_inference_memory": 143.89794921875,
        "lora_predict": "D",
        "lora_answer": "B",
        "lora_time": 1.039243459701538,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.27880859375,
        "lora_inference_memory": 143.89794921875
    },
    {
        "seed": 706267,
        "category": "high_school_statistics",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 1.258486032485962,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3225.42333984375,
        "base_inference_memory": 233.74560546875,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 1.3488054275512695,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3243.99365234375,
        "lora_inference_memory": 233.61279296875
    },
    {
        "seed": 777493,
        "category": "college_biology",
        "base_predict": "D",
        "base_answer": "C",
        "base_time": 1.2082149982452393,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3214.72021484375,
        "base_inference_memory": 223.04248046875,
        "lora_predict": "D",
        "lora_answer": "C",
        "lora_time": 1.3312976360321045,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3233.52490234375,
        "lora_inference_memory": 223.14404296875
    },
    {
        "seed": 429349,
        "category": "formal_logic",
        "base_predict": "B",
        "base_answer": "C",
        "base_time": 1.3683462142944336,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3316.25439453125,
        "base_inference_memory": 324.57666015625,
        "lora_predict": "B",
        "lora_answer": "C",
        "lora_time": 1.510089635848999,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3334.80908203125,
        "lora_inference_memory": 324.42822265625
    },
    {
        "seed": 589086,
        "category": "us_foreign_policy",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 1.0047805309295654,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3173.9052734375,
        "base_inference_memory": 182.2275390625,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 1.0814127922058105,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3191.6474609375,
        "lora_inference_memory": 181.2666015625
    },
    {
        "seed": 624973,
        "category": "high_school_mathematics",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 1.0942447185516357,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3248.97021484375,
        "base_inference_memory": 257.29248046875,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.1949865818023682,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3267.02490234375,
        "lora_inference_memory": 256.64404296875
    },
    {
        "seed": 321409,
        "category": "conceptual_physics",
        "base_predict": "C",
        "base_answer": "D",
        "base_time": 0.775841236114502,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.4150390625,
        "base_inference_memory": 143.7373046875,
        "lora_predict": "C",
        "lora_answer": "D",
        "lora_time": 1.162813663482666,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.1181640625,
        "lora_inference_memory": 143.7373046875
    },
    {
        "seed": 605666,
        "category": "astronomy",
        "base_predict": "D",
        "base_answer": "C",
        "base_time": 1.4797217845916748,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3206.15771484375,
        "base_inference_memory": 214.47998046875,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.5544040203094482,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3225.14990234375,
        "lora_inference_memory": 214.76904296875
    },
    {
        "seed": 962352,
        "category": "management",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 1.645570993423462,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.1279296875,
        "base_inference_memory": 142.4501953125,
        "lora_predict": "C",
        "lora_answer": "D",
        "lora_time": 1.5296704769134521,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3152.8310546875,
        "lora_inference_memory": 142.4501953125
    },
    {
        "seed": 200282,
        "category": "human_sexuality",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.63893723487854,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.6103515625,
        "base_inference_memory": 142.9326171875,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 1.2476465702056885,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3153.3134765625,
        "lora_inference_memory": 142.9326171875
    },
    {
        "seed": 196478,
        "category": "business_ethics",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 1.5138003826141357,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.28515625,
        "base_inference_memory": 152.607421875,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 1.4310383796691895,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.3818359375,
        "lora_inference_memory": 153.0009765625
    },
    {
        "seed": 540313,
        "category": "high_school_statistics",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 1.6039903163909912,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3263.95458984375,
        "base_inference_memory": 272.27685546875,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 1.564504861831665,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3281.68115234375,
        "lora_inference_memory": 271.30029296875
    },
    {
        "seed": 525458,
        "category": "business_ethics",
        "base_predict": "B",
        "base_answer": "C",
        "base_time": 1.15938401222229,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3176.1162109375,
        "base_inference_memory": 184.4384765625,
        "lora_predict": "B",
        "lora_answer": "C",
        "lora_time": 1.0777366161346436,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3194.7880859375,
        "lora_inference_memory": 184.4072265625
    },
    {
        "seed": 265289,
        "category": "business_ethics",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 1.1231436729431152,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3167.5537109375,
        "base_inference_memory": 175.8759765625,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 1.200268030166626,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3186.4130859375,
        "lora_inference_memory": 176.0322265625
    },
    {
        "seed": 742789,
        "category": "college_chemistry",
        "base_predict": "D",
        "base_answer": "A",
        "base_time": 1.2889370918273926,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3150.3583984375,
        "base_inference_memory": 158.6806640625,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 2.100822925567627,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3168.6162109375,
        "lora_inference_memory": 158.2353515625
    },
    {
        "seed": 476861,
        "category": "high_school_computer_science",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 2.2297937870025635,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3303.13720703125,
        "base_inference_memory": 311.45947265625,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 2.4698967933654785,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3320.99658203125,
        "lora_inference_memory": 310.61572265625
    },
    {
        "seed": 122491,
        "category": "college_computer_science",
        "base_predict": "C",
        "base_answer": "A",
        "base_time": 1.6863362789154053,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.70263671875,
        "base_inference_memory": 152.02490234375,
        "lora_predict": "C",
        "lora_answer": "A",
        "lora_time": 1.6012752056121826,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.40576171875,
        "lora_inference_memory": 152.02490234375
    },
    {
        "seed": 872616,
        "category": "high_school_mathematics",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 1.3063833713531494,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3178.2568359375,
        "base_inference_memory": 186.5791015625,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.2199506759643555,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3196.8818359375,
        "lora_inference_memory": 186.5009765625
    },
    {
        "seed": 96538,
        "category": "elementary_mathematics",
        "base_predict": "C",
        "base_answer": "B",
        "base_time": 1.6523325443267822,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3257.53271484375,
        "base_inference_memory": 265.85498046875,
        "lora_predict": "C",
        "lora_answer": "B",
        "lora_time": 1.3451175689697266,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3275.39990234375,
        "lora_inference_memory": 265.01904296875
    },
    {
        "seed": 236453,
        "category": "high_school_physics",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 1.2608566284179688,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3261.93115234375,
        "base_inference_memory": 270.25341796875,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 1.3550875186920166,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3280.63427734375,
        "lora_inference_memory": 270.25341796875
    },
    {
        "seed": 190049,
        "category": "professional_law",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.8483154773712158,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3477.61376953125,
        "base_inference_memory": 485.93603515625,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 2.0163166522979736,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3496.31689453125,
        "lora_inference_memory": 485.93603515625
    },
    {
        "seed": 273331,
        "category": "abstract_algebra",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 1.0078120231628418,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3169.6240234375,
        "base_inference_memory": 177.9462890625,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 1.1064908504486084,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3187.4599609375,
        "lora_inference_memory": 177.0791015625
    },
    {
        "seed": 985742,
        "category": "high_school_mathematics",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 0.8137321472167969,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.3125,
        "base_inference_memory": 146.634765625,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 0.8977701663970947,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.015625,
        "lora_inference_memory": 146.634765625
    },
    {
        "seed": 116197,
        "category": "marketing",
        "base_predict": "B",
        "base_answer": "B",
        "base_time": 1.1879017353057861,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3142.39306640625,
        "base_inference_memory": 150.71533203125,
        "lora_predict": "B",
        "lora_answer": "B",
        "lora_time": 1.2664332389831543,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3161.09619140625,
        "lora_inference_memory": 150.71533203125
    },
    {
        "seed": 218762,
        "category": "computer_security",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 0.8907444477081299,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3191.7880859375,
        "base_inference_memory": 200.1103515625,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 0.9914884567260742,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3210.4912109375,
        "lora_inference_memory": 200.1103515625
    },
    {
        "seed": 682398,
        "category": "high_school_computer_science",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.9561762809753418,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3242.54833984375,
        "base_inference_memory": 250.87060546875,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 1.0526494979858398,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3260.74365234375,
        "lora_inference_memory": 250.36279296875
    },
    {
        "seed": 449062,
        "category": "formal_logic",
        "base_predict": "D",
        "base_answer": "A",
        "base_time": 1.0240402221679688,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.1396484375,
        "base_inference_memory": 152.4619140625,
        "lora_predict": "D",
        "lora_answer": "A",
        "lora_time": 1.1003129482269287,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.6240234375,
        "lora_inference_memory": 153.2431640625
    },
    {
        "seed": 519375,
        "category": "moral_disputes",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 0.7794394493103027,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3183.4130859375,
        "base_inference_memory": 191.7353515625,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 0.8737328052520752,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3202.1162109375,
        "lora_inference_memory": 191.7353515625
    },
    {
        "seed": 896803,
        "category": "public_relations",
        "base_predict": "D",
        "base_answer": "B",
        "base_time": 1.4033761024475098,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3217.88037109375,
        "base_inference_memory": 226.20263671875,
        "lora_predict": "D",
        "lora_answer": "B",
        "lora_time": 0.7090189456939697,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3152.3486328125,
        "lora_inference_memory": 141.9677734375
    },
    {
        "seed": 355378,
        "category": "high_school_biology",
        "base_predict": "C",
        "base_answer": "A",
        "base_time": 0.7829501628875732,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3146.1474609375,
        "base_inference_memory": 154.4697265625,
        "lora_predict": "C",
        "lora_answer": "A",
        "lora_time": 0.8312554359436035,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3165.4755859375,
        "lora_inference_memory": 155.0947265625
    },
    {
        "seed": 93294,
        "category": "security_studies",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 0.6651885509490967,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3226.33740234375,
        "base_inference_memory": 234.65966796875,
        "lora_predict": "B",
        "lora_answer": "C",
        "lora_time": 0.7636668682098389,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3245.04052734375,
        "lora_inference_memory": 234.65966796875
    },
    {
        "seed": 823015,
        "category": "moral_scenarios",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 0.6283538341522217,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3249.36865234375,
        "base_inference_memory": 257.69091796875,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 0.7270894050598145,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3268.07177734375,
        "lora_inference_memory": 257.69091796875
    },
    {
        "seed": 256978,
        "category": "human_aging",
        "base_predict": "D",
        "base_answer": "A",
        "base_time": 0.4496021270751953,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.60107421875,
        "base_inference_memory": 147.92333984375,
        "lora_predict": "D",
        "lora_answer": "A",
        "lora_time": 0.586803674697876,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.92919921875,
        "lora_inference_memory": 148.54833984375
    },
    {
        "seed": 311733,
        "category": "conceptual_physics",
        "base_predict": "C",
        "base_answer": "D",
        "base_time": 0.53128981590271,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.41162109375,
        "base_inference_memory": 151.73388671875,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 0.6191215515136719,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.11474609375,
        "lora_inference_memory": 151.73388671875
    },
    {
        "seed": 941141,
        "category": "college_mathematics",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.45023083686828613,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3181.3193359375,
        "base_inference_memory": 189.6416015625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 0.5386066436767578,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3200.0224609375,
        "lora_inference_memory": 189.6416015625
    },
    {
        "seed": 448593,
        "category": "human_sexuality",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.35437631607055664,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.44970703125,
        "base_inference_memory": 142.77197265625,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 0.9893927574157715,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3153.15283203125,
        "lora_inference_memory": 142.77197265625
    },
    {
        "seed": 303211,
        "category": "astronomy",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 0.4278683662414551,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3216.86083984375,
        "base_inference_memory": 225.18310546875,
        "lora_predict": "A",
        "lora_answer": "A",
        "lora_time": 0.5367631912231445,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3235.61865234375,
        "lora_inference_memory": 225.23779296875
    },
    {
        "seed": 520166,
        "category": "clinical_knowledge",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 0.3890988826751709,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3167.4833984375,
        "base_inference_memory": 175.8056640625,
        "lora_predict": "A",
        "lora_answer": "D",
        "lora_time": 0.529000997543335,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3185.3662109375,
        "lora_inference_memory": 174.9853515625
    },
    {
        "seed": 885381,
        "category": "professional_law",
        "base_predict": "C",
        "base_answer": "B",
        "base_time": 0.922405481338501,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3899.3720703125,
        "base_inference_memory": 907.6943359375,
        "lora_predict": "D",
        "lora_answer": "B",
        "lora_time": 1.998425006866455,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3917.0830078125,
        "lora_inference_memory": 906.7021484375
    },
    {
        "seed": 246129,
        "category": "us_foreign_policy",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 4.139158010482788,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3325.703125,
        "base_inference_memory": 334.025390625,
        "lora_predict": "D",
        "lora_answer": "C",
        "lora_time": 1.3448641300201416,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3189.5537109375,
        "lora_inference_memory": 179.1728515625
    },
    {
        "seed": 323451,
        "category": "computer_security",
        "base_predict": "A",
        "base_answer": "A",
        "base_time": 1.1210589408874512,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3208.22802734375,
        "base_inference_memory": 216.55029296875,
        "lora_predict": "B",
        "lora_answer": "A",
        "lora_time": 1.2613122463226318,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3226.19677734375,
        "lora_inference_memory": 215.81591796875
    },
    {
        "seed": 567366,
        "category": "philosophy",
        "base_predict": "C",
        "base_answer": "B",
        "base_time": 1.1227381229400635,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.1396484375,
        "base_inference_memory": 152.4619140625,
        "lora_predict": "C",
        "lora_answer": "B",
        "lora_time": 1.0514857769012451,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.8427734375,
        "lora_inference_memory": 152.4619140625
    },
    {
        "seed": 452873,
        "category": "high_school_government_and_politics",
        "base_predict": "D",
        "base_answer": "D",
        "base_time": 1.0242273807525635,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3201.80615234375,
        "base_inference_memory": 210.12841796875,
        "lora_predict": "D",
        "lora_answer": "D",
        "lora_time": 0.9513413906097412,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3219.91552734375,
        "lora_inference_memory": 209.53466796875
    },
    {
        "seed": 824155,
        "category": "nutrition",
        "base_predict": "A",
        "base_answer": "C",
        "base_time": 1.0172395706176758,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3161.1318359375,
        "base_inference_memory": 169.4541015625,
        "lora_predict": "A",
        "lora_answer": "C",
        "lora_time": 1.0568933486938477,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3180.1318359375,
        "lora_inference_memory": 169.7509765625
    },
    {
        "seed": 856841,
        "category": "high_school_european_history",
        "base_predict": "A",
        "base_answer": "D",
        "base_time": 1.8878722190856934,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3549.08544921875,
        "base_inference_memory": 557.40771484375,
        "lora_predict": "C",
        "lora_answer": "D",
        "lora_time": 1.9100158214569092,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3567.50732421875,
        "lora_inference_memory": 557.12646484375
    },
    {
        "seed": 651765,
        "category": "world_religions",
        "base_predict": "A",
        "base_answer": "B",
        "base_time": 0.7758681774139404,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3136.38037109375,
        "base_inference_memory": 144.70263671875,
        "lora_predict": "A",
        "lora_answer": "B",
        "lora_time": 0.9603841304779053,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3155.08349609375,
        "lora_inference_memory": 144.70263671875
    },
    {
        "seed": 138760,
        "category": "high_school_computer_science",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 2.107834815979004,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3377.7333984375,
        "base_inference_memory": 386.0556640625,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 2.0750746726989746,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3396.4365234375,
        "lora_inference_memory": 386.0556640625
    },
    {
        "seed": 322683,
        "category": "college_biology",
        "base_predict": "D",
        "base_answer": "A",
        "base_time": 1.678912878036499,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3282.10595703125,
        "base_inference_memory": 290.42822265625,
        "lora_predict": "D",
        "lora_answer": "A",
        "lora_time": 1.737011194229126,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3300.80908203125,
        "lora_inference_memory": 290.42822265625
    },
    {
        "seed": 717277,
        "category": "college_biology",
        "base_predict": "D",
        "base_answer": "B",
        "base_time": 1.044731855392456,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.43994140625,
        "base_inference_memory": 147.76220703125,
        "lora_predict": "C",
        "lora_answer": "B",
        "lora_time": 1.233412742614746,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.14306640625,
        "lora_inference_memory": 147.76220703125
    },
    {
        "seed": 614582,
        "category": "us_foreign_policy",
        "base_predict": "C",
        "base_answer": "C",
        "base_time": 1.2782962322235107,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.70263671875,
        "base_inference_memory": 152.02490234375,
        "lora_predict": "C",
        "lora_answer": "C",
        "lora_time": 1.2115533351898193,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.40576171875,
        "lora_inference_memory": 152.02490234375
    }
]