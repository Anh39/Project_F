[
    {
        "model_name": "gemma2b"
    },
    {
        "seed": 733443,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.9836773872375488,
        "base_initial_memory": 1975.2744140625,
        "base_end_memory": 2280.0283203125,
        "base_inference_memory": 304.75390625,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.9241139888763428,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3213.2763671875,
        "lora_inference_memory": 221.5986328125
    },
    {
        "seed": 643319,
        "category": "high_school_world_history",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 1.1362261772155762,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3437.2275390625,
        "base_inference_memory": 445.5498046875,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 1.4021880626678467,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3455.9306640625,
        "lora_inference_memory": 464.2529296875
    },
    {
        "seed": 973184,
        "category": "professional_accounting",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.6581375598907471,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3246.6162109375,
        "base_inference_memory": 254.9384765625,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.9037573337554932,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3265.3193359375,
        "lora_inference_memory": 273.6416015625
    },
    {
        "seed": 814486,
        "category": "machine_learning",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.726107120513916,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3153.2041015625,
        "base_inference_memory": 161.5263671875,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.9003596305847168,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3171.2275390625,
        "lora_inference_memory": 179.5498046875
    },
    {
        "seed": 666421,
        "category": "college_medicine",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.5945942401885986,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3206.7099609375,
        "base_inference_memory": 215.0322265625,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.8376548290252686,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3224.4912109375,
        "lora_inference_memory": 232.8134765625
    },
    {
        "seed": 240603,
        "category": "philosophy",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.33956098556518555,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.0244140625,
        "base_inference_memory": 145.3466796875,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.5471222400665283,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3155.7275390625,
        "lora_inference_memory": 164.0498046875
    },
    {
        "seed": 906286,
        "category": "abstract_algebra",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.6081242561340332,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.86767578125,
        "base_inference_memory": 153.18994140625,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.8318438529968262,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3163.57080078125,
        "lora_inference_memory": 171.89306640625
    },
    {
        "seed": 13510,
        "category": "management",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.3842771053314209,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.28857421875,
        "base_inference_memory": 142.61083984375,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.5704281330108643,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3152.99169921875,
        "lora_inference_memory": 161.31396484375
    },
    {
        "seed": 660042,
        "category": "college_physics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 0.5199136734008789,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3200.4521484375,
        "base_inference_memory": 208.7744140625,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 0.7403998374938965,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3218.3740234375,
        "lora_inference_memory": 226.6962890625
    },
    {
        "seed": 645177,
        "category": "professional_medicine",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.688800573348999,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3334.2529296875,
        "base_inference_memory": 342.5751953125,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.9158799648284912,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3352.9560546875,
        "lora_inference_memory": 361.2783203125
    },
    {
        "seed": 927802,
        "category": "high_school_macroeconomics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 0,
        "base_time": 0.2945742607116699,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3140.08447265625,
        "base_inference_memory": 148.40673828125,
        "lora_predict": 3,
        "lora_answer": 0,
        "lora_time": 0.4832746982574463,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3158.78759765625,
        "lora_inference_memory": 167.10986328125
    },
    {
        "seed": 106207,
        "category": "high_school_chemistry",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.45477938652038574,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3226.3818359375,
        "base_inference_memory": 234.7041015625,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6658852100372314,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3245.0849609375,
        "lora_inference_memory": 253.4072265625
    },
    {
        "seed": 214904,
        "category": "sociology",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.45000791549682617,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3146.9931640625,
        "base_inference_memory": 155.3154296875,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.6391451358795166,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3165.1572265625,
        "lora_inference_memory": 173.4794921875
    },
    {
        "seed": 459466,
        "category": "medical_genetics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 0.25554895401000977,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.18505859375,
        "base_inference_memory": 145.50732421875,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 0.46166515350341797,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3155.88818359375,
        "lora_inference_memory": 164.21044921875
    },
    {
        "seed": 836791,
        "category": "astronomy",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.329509973526001,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3170.7353515625,
        "base_inference_memory": 179.0576171875,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.527660608291626,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3188.8056640625,
        "lora_inference_memory": 197.1279296875
    },
    {
        "seed": 342445,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.3116888999938965,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3168.7119140625,
        "base_inference_memory": 177.0341796875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.521761417388916,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3186.7666015625,
        "lora_inference_memory": 195.0888671875
    },
    {
        "seed": 589007,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.3578200340270996,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3229.8779296875,
        "base_inference_memory": 238.2001953125,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.545621395111084,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3247.9404296875,
        "lora_inference_memory": 256.2626953125
    },
    {
        "seed": 414198,
        "category": "professional_medicine",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.4754481315612793,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3387.2705078125,
        "base_inference_memory": 395.5927734375,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.722583532333374,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3405.9736328125,
        "lora_inference_memory": 414.2958984375
    },
    {
        "seed": 68290,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.19707489013671875,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.95654296875,
        "base_inference_memory": 147.27880859375,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.4002349376678467,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3157.65966796875,
        "lora_inference_memory": 165.98193359375
    },
    {
        "seed": 454013,
        "category": [
            "professional_psychology"
        ],
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.18519210815429688,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.1513671875,
        "base_inference_memory": 146.4736328125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.371767520904541,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3156.8544921875,
        "lora_inference_memory": 165.1767578125
    },
    {
        "seed": 46801,
        "category": "high_school_biology",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.31351733207702637,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3213.0107421875,
        "base_inference_memory": 221.3330078125,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 1.1205992698669434,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3231.6279296875,
        "lora_inference_memory": 239.9501953125
    },
    {
        "seed": 174546,
        "category": "high_school_computer_science",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.5027387142181396,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.95654296875,
        "base_inference_memory": 147.27880859375,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.698932409286499,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3157.65966796875,
        "lora_inference_memory": 165.98193359375
    },
    {
        "seed": 181507,
        "category": "astronomy",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.4841322898864746,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.3125,
        "base_inference_memory": 146.634765625,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.5486598014831543,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3157.015625,
        "lora_inference_memory": 165.337890625
    },
    {
        "seed": 889259,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 3.5657665729522705,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3753.7998046875,
        "base_inference_memory": 762.1220703125,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 3.3226935863494873,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3771.9951171875,
        "lora_inference_memory": 780.3173828125
    },
    {
        "seed": 403027,
        "category": "high_school_world_history",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.7163982391357422,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.8291015625,
        "base_inference_memory": 146.1513671875,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.6132757663726807,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3156.5322265625,
        "lora_inference_memory": 164.8544921875
    },
    {
        "seed": 596944,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.40408802032470703,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.63427734375,
        "base_inference_memory": 146.95654296875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.5853290557861328,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3157.33740234375,
        "lora_inference_memory": 165.65966796875
    },
    {
        "seed": 473964,
        "category": "philosophy",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.7738258838653564,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3149.0634765625,
        "base_inference_memory": 157.3857421875,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.840177059173584,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3167.1806640625,
        "lora_inference_memory": 175.5029296875
    },
    {
        "seed": 799993,
        "category": "us_foreign_policy",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.4697902202606201,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3172.7587890625,
        "base_inference_memory": 181.0810546875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6611261367797852,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3190.8447265625,
        "lora_inference_memory": 199.1669921875
    },
    {
        "seed": 316225,
        "category": "prehistory",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 1,
        "base_time": 0.3453800678253174,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3140.2451171875,
        "base_inference_memory": 148.5673828125,
        "lora_predict": 0,
        "lora_answer": 1,
        "lora_time": 0.5351693630218506,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3158.9482421875,
        "lora_inference_memory": 167.2705078125
    },
    {
        "seed": 334015,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.3398432731628418,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.8974609375,
        "base_inference_memory": 144.2197265625,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.533193826675415,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3154.6005859375,
        "lora_inference_memory": 162.9228515625
    },
    {
        "seed": 371559,
        "category": "management",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.9206407070159912,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3393.3876953125,
        "base_inference_memory": 401.7099609375,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 1.154360294342041,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3412.0908203125,
        "lora_inference_memory": 420.4130859375
    },
    {
        "seed": 659412,
        "category": "high_school_computer_science",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.32488107681274414,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3140.08447265625,
        "base_inference_memory": 148.40673828125,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.5287084579467773,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3158.78759765625,
        "lora_inference_memory": 167.10986328125
    },
    {
        "seed": 652050,
        "category": "global_facts",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.520416259765625,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3234.0185546875,
        "base_inference_memory": 242.3408203125,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.7424242496490479,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3252.1669921875,
        "lora_inference_memory": 260.4892578125
    },
    {
        "seed": 468577,
        "category": "moral_scenarios",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.5650157928466797,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3151.1337890625,
        "base_inference_memory": 159.4560546875,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.7702114582061768,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3169.2041015625,
        "lora_inference_memory": 177.5263671875
    },
    {
        "seed": 618281,
        "category": "sociology",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.5571441650390625,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.00244140625,
        "base_inference_memory": 151.32470703125,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.7260069847106934,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.70556640625,
        "lora_inference_memory": 170.02783203125
    },
    {
        "seed": 136648,
        "category": "us_foreign_policy",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.4932065010070801,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3217.4560546875,
        "base_inference_memory": 225.7783203125,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.6149656772613525,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3235.7060546875,
        "lora_inference_memory": 244.0283203125
    },
    {
        "seed": 486215,
        "category": "high_school_chemistry",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.23675012588500977,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.34619140625,
        "base_inference_memory": 145.66845703125,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.4221484661102295,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3156.04931640625,
        "lora_inference_memory": 164.37158203125
    },
    {
        "seed": 304336,
        "category": "global_facts",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.32956933975219727,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3142.82958984375,
        "base_inference_memory": 151.15185546875,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.5080912113189697,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.53271484375,
        "lora_inference_memory": 169.85498046875
    },
    {
        "seed": 471553,
        "category": "public_relations",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.23191428184509277,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.44970703125,
        "base_inference_memory": 142.77197265625,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.9065544605255127,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3153.15283203125,
        "lora_inference_memory": 161.47509765625
    },
    {
        "seed": 642,
        "category": "philosophy",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.25739169120788574,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3176.2197265625,
        "base_inference_memory": 184.5419921875,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.9451074600219727,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3194.9228515625,
        "lora_inference_memory": 203.2451171875
    },
    {
        "seed": 264316,
        "category": "us_foreign_policy",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.21007871627807617,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.60107421875,
        "base_inference_memory": 147.92333984375,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.9278392791748047,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3158.30419921875,
        "lora_inference_memory": 166.62646484375
    },
    {
        "seed": 16327,
        "category": "high_school_geography",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.8582944869995117,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3153.2041015625,
        "base_inference_memory": 161.5263671875,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 1.36124587059021,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3171.2275390625,
        "lora_inference_memory": 179.5498046875
    },
    {
        "seed": 224314,
        "category": "clinical_knowledge",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.8566555976867676,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3204.7685546875,
        "base_inference_memory": 213.0908203125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 1.362396478652954,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3223.4716796875,
        "lora_inference_memory": 231.7939453125
    },
    {
        "seed": 598376,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.8246209621429443,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.1279296875,
        "base_inference_memory": 142.4501953125,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.6430530548095703,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3152.8310546875,
        "lora_inference_memory": 161.1533203125
    },
    {
        "seed": 822744,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.612480878829956,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3230.4287109375,
        "base_inference_memory": 238.7509765625,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 1.737863540649414,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3248.9599609375,
        "lora_inference_memory": 257.2822265625
    },
    {
        "seed": 290223,
        "category": "nutrition",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 1.2667295932769775,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3224.3583984375,
        "base_inference_memory": 232.6806640625,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 1.7048041820526123,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3243.0615234375,
        "lora_inference_memory": 251.3837890625
    },
    {
        "seed": 992324,
        "category": "high_school_physics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.7029659748077393,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.4150390625,
        "base_inference_memory": 143.7373046875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.8743088245391846,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3154.1181640625,
        "lora_inference_memory": 162.4404296875
    },
    {
        "seed": 200649,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 3.3270983695983887,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3628.9052734375,
        "base_inference_memory": 637.2275390625,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 3.2829606533050537,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3647.6083984375,
        "lora_inference_memory": 655.9306640625
    },
    {
        "seed": 683417,
        "category": "high_school_us_history",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.884838342666626,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3159.6064453125,
        "base_inference_memory": 167.9287109375,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.882305383682251,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3178.3095703125,
        "lora_inference_memory": 186.6318359375
    },
    {
        "seed": 426580,
        "category": "jurisprudence",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.8195476531982422,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3190.4951171875,
        "base_inference_memory": 198.8173828125,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 1.005488395690918,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3209.1982421875,
        "lora_inference_memory": 217.5205078125
    },
    {
        "seed": 487361,
        "category": "high_school_microeconomics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.9292786121368408,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3147.4658203125,
        "base_inference_memory": 155.7880859375,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 1.1234076023101807,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3166.1689453125,
        "lora_inference_memory": 174.4912109375
    },
    {
        "seed": 723459,
        "category": "college_biology",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.4371943473815918,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.0244140625,
        "base_inference_memory": 145.3466796875,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.6329185962677002,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3155.7275390625,
        "lora_inference_memory": 164.0498046875
    },
    {
        "seed": 788373,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.5695478916168213,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3134.44970703125,
        "base_inference_memory": 142.77197265625,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.6154172420501709,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3153.15283203125,
        "lora_inference_memory": 161.47509765625
    },
    {
        "seed": 548742,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.637178897857666,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3149.0634765625,
        "base_inference_memory": 157.3857421875,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.7903573513031006,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3167.1806640625,
        "lora_inference_memory": 175.5029296875
    },
    {
        "seed": 376357,
        "category": "machine_learning",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.6727566719055176,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3288.1494140625,
        "base_inference_memory": 296.4716796875,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.9046285152435303,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3306.0556640625,
        "lora_inference_memory": 314.3779296875
    },
    {
        "seed": 789611,
        "category": "college_biology",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.5587289333343506,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3142.53857421875,
        "base_inference_memory": 150.86083984375,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.7633547782897949,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.24169921875,
        "lora_inference_memory": 169.56396484375
    },
    {
        "seed": 443062,
        "category": "management",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.3058476448059082,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.18505859375,
        "base_inference_memory": 145.50732421875,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.4975295066833496,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3155.88818359375,
        "lora_inference_memory": 164.21044921875
    },
    {
        "seed": 652210,
        "category": "high_school_computer_science",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.4770679473876953,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3218.2880859375,
        "base_inference_memory": 226.6103515625,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.6773207187652588,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3236.9912109375,
        "lora_inference_memory": 245.3134765625
    },
    {
        "seed": 819021,
        "category": "professional_accounting",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 0,
        "base_time": 0.4437382221221924,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3219.5263671875,
        "base_inference_memory": 227.8486328125,
        "lora_predict": 3,
        "lora_answer": 0,
        "lora_time": 0.6348960399627686,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3238.0029296875,
        "lora_inference_memory": 246.3251953125
    },
    {
        "seed": 641716,
        "category": "college_biology",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.42261171340942383,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3244.5927734375,
        "base_inference_memory": 252.9150390625,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.6085968017578125,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3263.2958984375,
        "lora_inference_memory": 271.6181640625
    },
    {
        "seed": 939261,
        "category": "high_school_statistics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 0.3250899314880371,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3159.4150390625,
        "base_inference_memory": 167.7373046875,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 0.5160098075866699,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3177.2978515625,
        "lora_inference_memory": 185.6201171875
    },
    {
        "seed": 486083,
        "category": "machine_learning",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.30214667320251465,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3168.7119140625,
        "base_inference_memory": 177.0341796875,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.5008969306945801,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3186.7666015625,
        "lora_inference_memory": 195.0888671875
    },
    {
        "seed": 777404,
        "category": "machine_learning",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.32320165634155273,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3145.01318359375,
        "base_inference_memory": 153.33544921875,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.5261826515197754,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3163.71630859375,
        "lora_inference_memory": 172.03857421875
    },
    {
        "seed": 417541,
        "category": "elementary_mathematics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.2773900032043457,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3161.6298828125,
        "base_inference_memory": 169.9521484375,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.46082377433776855,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3180.3330078125,
        "lora_inference_memory": 188.6552734375
    },
    {
        "seed": 912781,
        "category": "moral_disputes",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.2071990966796875,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3133.80615234375,
        "base_inference_memory": 142.12841796875,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.6433331966400146,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3152.50927734375,
        "lora_inference_memory": 160.83154296875
    },
    {
        "seed": 485289,
        "category": "anatomy",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.8410849571228027,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3234.0185546875,
        "base_inference_memory": 242.3408203125,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 1.6740264892578125,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3252.1669921875,
        "lora_inference_memory": 260.4892578125
    },
    {
        "seed": 389017,
        "category": "moral_scenarios",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 1.841257095336914,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3321.5693359375,
        "base_inference_memory": 329.8916015625,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 2.345304489135742,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3339.7021484375,
        "lora_inference_memory": 348.0244140625
    },
    {
        "seed": 274815,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 1.5019299983978271,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3155.5595703125,
        "base_inference_memory": 163.8818359375,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 1.2244627475738525,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3174.2626953125,
        "lora_inference_memory": 182.5849609375
    },
    {
        "seed": 38975,
        "category": "high_school_mathematics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.6438262462615967,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3168.7119140625,
        "base_inference_memory": 177.0341796875,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.6835739612579346,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3186.7666015625,
        "lora_inference_memory": 195.0888671875
    },
    {
        "seed": 625860,
        "category": "college_medicine",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.35462284088134766,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.8974609375,
        "base_inference_memory": 144.2197265625,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.5670857429504395,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3154.6005859375,
        "lora_inference_memory": 162.9228515625
    },
    {
        "seed": 127214,
        "category": "logical_fallacies",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.4760713577270508,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3170.7353515625,
        "base_inference_memory": 179.0576171875,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.6877720355987549,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3188.8056640625,
        "lora_inference_memory": 197.1279296875
    },
    {
        "seed": 767377,
        "category": "logical_fallacies",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.5525555610656738,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3215.3857421875,
        "base_inference_memory": 223.7080078125,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.7512202262878418,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3233.9560546875,
        "lora_inference_memory": 242.2783203125
    },
    {
        "seed": 862887,
        "category": "high_school_mathematics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.31920886039733887,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3136.21923828125,
        "base_inference_memory": 144.54150390625,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.5075478553771973,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3154.92236328125,
        "lora_inference_memory": 163.24462890625
    },
    {
        "seed": 507619,
        "category": "philosophy",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 1,
        "base_time": 0.31064915657043457,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.47314453125,
        "base_inference_memory": 146.79541015625,
        "lora_predict": 0,
        "lora_answer": 1,
        "lora_time": 0.50813889503479,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3157.17626953125,
        "lora_inference_memory": 165.49853515625
    },
    {
        "seed": 256212,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.5205078125,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.14599609375,
        "base_inference_memory": 151.46826171875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.7239334583282471,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.84912109375,
        "lora_inference_memory": 170.17138671875
    },
    {
        "seed": 154382,
        "category": "clinical_knowledge",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.6072249412536621,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3261.0322265625,
        "base_inference_memory": 269.3544921875,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.7754590511322021,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3279.5478515625,
        "lora_inference_memory": 287.8701171875
    },
    {
        "seed": 96733,
        "category": "high_school_government_and_politics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 1.106332540512085,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3722.7060546875,
        "base_inference_memory": 731.0283203125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 1.381059169769287,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3742.2060546875,
        "lora_inference_memory": 750.5283203125
    },
    {
        "seed": 754812,
        "category": "high_school_world_history",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.4073638916015625,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3142.82958984375,
        "base_inference_memory": 151.15185546875,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.588336706161499,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.53271484375,
        "lora_inference_memory": 169.85498046875
    },
    {
        "seed": 75590,
        "category": "high_school_macroeconomics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.39661478996276855,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3155.2744140625,
        "base_inference_memory": 163.5966796875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6021332740783691,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3173.2509765625,
        "lora_inference_memory": 181.5732421875
    },
    {
        "seed": 662726,
        "category": "anatomy",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.3676872253417969,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3155.2744140625,
        "base_inference_memory": 163.5966796875,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.5496416091918945,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3173.2509765625,
        "lora_inference_memory": 181.5732421875
    },
    {
        "seed": 346923,
        "category": "high_school_macroeconomics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.3311588764190674,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3209.8662109375,
        "base_inference_memory": 218.1884765625,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.5197153091430664,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3228.5693359375,
        "lora_inference_memory": 236.8916015625
    },
    {
        "seed": 946474,
        "category": "college_biology",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.39600491523742676,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3305.7041015625,
        "base_inference_memory": 314.0263671875,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.5984444618225098,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3324.4072265625,
        "lora_inference_memory": 332.7294921875
    },
    {
        "seed": 575419,
        "category": "high_school_statistics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.43964576721191406,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3430.1689453125,
        "base_inference_memory": 438.4912109375,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 1.077864646911621,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3448.7939453125,
        "lora_inference_memory": 457.1162109375
    },
    {
        "seed": 460356,
        "category": "high_school_european_history",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.35872983932495117,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3336.2919921875,
        "base_inference_memory": 344.6142578125,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.5794737339019775,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3354.9951171875,
        "lora_inference_memory": 363.3173828125
    },
    {
        "seed": 587475,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.18447279930114746,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.8291015625,
        "base_inference_memory": 146.1513671875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.8746678829193115,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3156.5322265625,
        "lora_inference_memory": 164.8544921875
    },
    {
        "seed": 656027,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.5626189708709717,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3142.68408203125,
        "base_inference_memory": 151.00634765625,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.8237478733062744,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.38720703125,
        "lora_inference_memory": 169.70947265625
    },
    {
        "seed": 119764,
        "category": "sociology",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.6424341201782227,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3155.5595703125,
        "base_inference_memory": 163.8818359375,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.9510595798492432,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3174.2626953125,
        "lora_inference_memory": 182.5849609375
    },
    {
        "seed": 849370,
        "category": "astronomy",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.39446234703063965,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.50732421875,
        "base_inference_memory": 145.82958984375,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.5602474212646484,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3156.21044921875,
        "lora_inference_memory": 164.53271484375
    },
    {
        "seed": 377157,
        "category": "astronomy",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 1.1941039562225342,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3486.5341796875,
        "base_inference_memory": 494.8564453125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 1.8045132160186768,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3504.8701171875,
        "lora_inference_memory": 513.1923828125
    },
    {
        "seed": 757692,
        "category": "professional_medicine",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.7118782997131348,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.76220703125,
        "base_inference_memory": 148.08447265625,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.9335412979125977,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3158.46533203125,
        "lora_inference_memory": 166.78759765625
    },
    {
        "seed": 748856,
        "category": "abstract_algebra",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.6929693222045898,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3133.966796875,
        "base_inference_memory": 142.2890625,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.7191216945648193,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3152.669921875,
        "lora_inference_memory": 160.9921875
    },
    {
        "seed": 103658,
        "category": "human_sexuality",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.7719783782958984,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.84814453125,
        "base_inference_memory": 152.17041015625,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 1.0948877334594727,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3162.55126953125,
        "lora_inference_memory": 170.87353515625
    },
    {
        "seed": 910406,
        "category": "high_school_government_and_politics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.7342336177825928,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3147.4658203125,
        "base_inference_memory": 155.7880859375,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 1.0660960674285889,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3166.1689453125,
        "lora_inference_memory": 174.4912109375
    },
    {
        "seed": 89949,
        "category": "jurisprudence",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.5432696342468262,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3133.001953125,
        "base_inference_memory": 141.32421875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6203680038452148,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3151.705078125,
        "lora_inference_memory": 160.02734375
    },
    {
        "seed": 235108,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.6564316749572754,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3155.5595703125,
        "base_inference_memory": 163.8818359375,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 1.0083608627319336,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3174.2626953125,
        "lora_inference_memory": 182.5849609375
    },
    {
        "seed": 752624,
        "category": "elementary_mathematics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.47704601287841797,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3142.53857421875,
        "base_inference_memory": 150.86083984375,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.6563780307769775,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3161.24169921875,
        "lora_inference_memory": 169.56396484375
    },
    {
        "seed": 430977,
        "category": "formal_logic",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.4307081699371338,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3250.5810546875,
        "base_inference_memory": 258.9033203125,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.6377501487731934,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3268.3310546875,
        "lora_inference_memory": 276.6533203125
    },
    {
        "seed": 82680,
        "category": "high_school_statistics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.3055851459503174,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3132.35888671875,
        "base_inference_memory": 140.68115234375,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.4896855354309082,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3151.06201171875,
        "lora_inference_memory": 159.38427734375
    },
    {
        "seed": 393994,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.3901181221008301,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3236.4990234375,
        "base_inference_memory": 244.8212890625,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6113870143890381,
        "lora_initial_memory": 2991.677734375,
        "lora_end_memory": 3255.2021484375,
        "lora_inference_memory": 263.5244140625
    }
]