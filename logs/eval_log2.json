[
    {
        "model_name": "gemma2b"
    },
    {
        "seed": 166962,
        "category": "machine_learning",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 1.016845941543579,
        "base_initial_memory": 1975.2744140625,
        "base_end_memory": 2258.3857421875,
        "base_inference_memory": 283.111328125,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.5304293632507324,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3199.0009765625,
        "lora_inference_memory": 188.6201171875
    },
    {
        "seed": 594802,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.7658922672271729,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3292.3212890625,
        "base_inference_memory": 300.6435546875,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.7750818729400635,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3310.1337890625,
        "lora_inference_memory": 299.7529296875
    },
    {
        "seed": 612975,
        "category": "college_physics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.5566737651824951,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3221.5966796875,
        "base_inference_memory": 229.9189453125,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.5842936038970947,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3240.0263671875,
        "lora_inference_memory": 229.6455078125
    },
    {
        "seed": 727014,
        "category": "high_school_biology",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.5229909420013428,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3196.6123046875,
        "base_inference_memory": 204.9345703125,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.5499691963195801,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3215.3154296875,
        "lora_inference_memory": 204.9345703125
    },
    {
        "seed": 572670,
        "category": "high_school_statistics",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.5219337940216064,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3219.5263671875,
        "base_inference_memory": 227.8486328125,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.5209314823150635,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3238.0029296875,
        "lora_inference_memory": 227.6220703125
    },
    {
        "seed": 102862,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.4083211421966553,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3182.3369140625,
        "base_inference_memory": 190.6591796875,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.4124717712402344,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3201.0400390625,
        "lora_inference_memory": 190.6591796875
    },
    {
        "seed": 928830,
        "category": "nutrition",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.4558391571044922,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3248.6396484375,
        "base_inference_memory": 256.9619140625,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.5016403198242188,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3267.3427734375,
        "lora_inference_memory": 256.9619140625
    },
    {
        "seed": 777763,
        "category": [
            "professional_psychology"
        ],
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.4333786964416504,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3230.4287109375,
        "base_inference_memory": 238.7509765625,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.46712613105773926,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3249.1318359375,
        "lora_inference_memory": 238.7509765625
    },
    {
        "seed": 656753,
        "category": "moral_scenarios",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.4050171375274658,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3248.5107421875,
        "base_inference_memory": 256.8330078125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.4448540210723877,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3266.3310546875,
        "lora_inference_memory": 255.9501953125
    },
    {
        "seed": 98503,
        "category": "high_school_statistics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.36599183082580566,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.30322265625,
        "base_inference_memory": 151.62548828125,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.3866598606109619,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.00634765625,
        "lora_inference_memory": 151.62548828125
    },
    {
        "seed": 541855,
        "category": "clinical_knowledge",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.3583695888519287,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3200.6904296875,
        "base_inference_memory": 209.0126953125,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.4023580551147461,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3219.3935546875,
        "lora_inference_memory": 209.0126953125
    },
    {
        "seed": 258718,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.349257230758667,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3204.7685546875,
        "base_inference_memory": 213.0908203125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.36786842346191406,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3223.4716796875,
        "lora_inference_memory": 213.0908203125
    },
    {
        "seed": 860720,
        "category": "professional_medicine",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.21038532257080078,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.1513671875,
        "base_inference_memory": 146.4736328125,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.23355531692504883,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3156.8544921875,
        "lora_inference_memory": 146.4736328125
    },
    {
        "seed": 368344,
        "category": "marketing",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.2955291271209717,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3190.0224609375,
        "base_inference_memory": 198.3447265625,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.3294944763183594,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3208.1787109375,
        "lora_inference_memory": 197.7978515625
    },
    {
        "seed": 418045,
        "category": "high_school_macroeconomics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 0.19565320014953613,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.34619140625,
        "base_inference_memory": 145.66845703125,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.2156667709350586,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3156.04931640625,
        "lora_inference_memory": 145.66845703125
    },
    {
        "seed": 567975,
        "category": "human_sexuality",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.1866769790649414,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.63427734375,
        "base_inference_memory": 146.95654296875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.21642494201660156,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.33740234375,
        "lora_inference_memory": 146.95654296875
    },
    {
        "seed": 965376,
        "category": "logical_fallacies",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.5210628509521484,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3566.7119140625,
        "base_inference_memory": 575.0341796875,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.6152355670928955,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3585.4150390625,
        "lora_inference_memory": 575.0341796875
    },
    {
        "seed": 376697,
        "category": "high_school_us_history",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.33487844467163086,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3275.6337890625,
        "base_inference_memory": 283.9560546875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.3913395404815674,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3293.8212890625,
        "lora_inference_memory": 283.4404296875
    },
    {
        "seed": 108637,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.6260952949523926,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3699.5205078125,
        "base_inference_memory": 707.8427734375,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.7353627681732178,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3717.9580078125,
        "lora_inference_memory": 707.5771484375
    },
    {
        "seed": 702247,
        "category": "professional_medicine",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.6295228004455566,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3457.2880859375,
        "base_inference_memory": 465.6103515625,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 2.7907490730285645,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3475.3037109375,
        "lora_inference_memory": 464.9228515625
    },
    {
        "seed": 761207,
        "category": "high_school_computer_science",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 1.1104485988616943,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3161.4853515625,
        "base_inference_memory": 169.8076171875,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.676584005355835,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3179.3212890625,
        "lora_inference_memory": 168.9404296875
    },
    {
        "seed": 733941,
        "category": "clinical_knowledge",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 0,
        "base_time": 0.9140028953552246,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3153.2041015625,
        "base_inference_memory": 161.5263671875,
        "lora_predict": 3,
        "lora_answer": 0,
        "lora_time": 0.9127786159515381,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3171.2275390625,
        "lora_inference_memory": 160.8466796875
    },
    {
        "seed": 333115,
        "category": "philosophy",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.604419469833374,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3180.2978515625,
        "base_inference_memory": 188.6201171875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6297481060028076,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3199.0009765625,
        "lora_inference_memory": 188.6201171875
    },
    {
        "seed": 757371,
        "category": "college_mathematics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.4118502140045166,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.57568359375,
        "base_inference_memory": 143.89794921875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.4214205741882324,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.27880859375,
        "lora_inference_memory": 143.89794921875
    },
    {
        "seed": 891950,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.39998865127563477,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3136.7021484375,
        "base_inference_memory": 145.0244140625,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.3779330253601074,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3155.4052734375,
        "lora_inference_memory": 145.0244140625
    },
    {
        "seed": 990504,
        "category": "world_religions",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.6070752143859863,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3194.1943359375,
        "base_inference_memory": 202.5166015625,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.6192598342895508,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3212.2568359375,
        "lora_inference_memory": 201.8759765625
    },
    {
        "seed": 219124,
        "category": "college_computer_science",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.5747189521789551,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3188.4560546875,
        "base_inference_memory": 196.7783203125,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.5822386741638184,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3207.1591796875,
        "lora_inference_memory": 196.7783203125
    },
    {
        "seed": 399649,
        "category": "astronomy",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.4415891170501709,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3163.6533203125,
        "base_inference_memory": 171.9755859375,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.4719843864440918,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3182.3564453125,
        "lora_inference_memory": 171.9755859375
    },
    {
        "seed": 885743,
        "category": "high_school_biology",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.6759722232818604,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3276.1376953125,
        "base_inference_memory": 284.4599609375,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.6926500797271729,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3294.8408203125,
        "lora_inference_memory": 284.4599609375
    },
    {
        "seed": 126589,
        "category": "formal_logic",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.523578405380249,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.86767578125,
        "base_inference_memory": 153.18994140625,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.5453188419342041,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.57080078125,
        "lora_inference_memory": 153.18994140625
    },
    {
        "seed": 743797,
        "category": "prehistory",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 1.2781999111175537,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3710.4716796875,
        "base_inference_memory": 718.7939453125,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 1.2895152568817139,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3730.1591796875,
        "lora_inference_memory": 719.7783203125
    },
    {
        "seed": 316149,
        "category": "high_school_us_history",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.44315552711486816,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3250.5810546875,
        "base_inference_memory": 258.9033203125,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.4715127944946289,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3268.3310546875,
        "lora_inference_memory": 257.9501953125
    },
    {
        "seed": 832110,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.49849867820739746,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3286.0634765625,
        "base_inference_memory": 294.3857421875,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.5498502254486084,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3304.0166015625,
        "lora_inference_memory": 293.6357421875
    },
    {
        "seed": 927725,
        "category": "elementary_mathematics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 0,
        "base_time": 0.6131877899169922,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3440.5986328125,
        "base_inference_memory": 448.9208984375,
        "lora_predict": 3,
        "lora_answer": 0,
        "lora_time": 0.6541321277618408,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3458.9892578125,
        "lora_inference_memory": 448.6083984375
    },
    {
        "seed": 630702,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.2199256420135498,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3136.21923828125,
        "base_inference_memory": 144.54150390625,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.3069908618927002,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.92236328125,
        "lora_inference_memory": 144.54150390625
    },
    {
        "seed": 206356,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.5138134956359863,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3151.5126953125,
        "base_inference_memory": 159.8349609375,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.7900378704071045,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3170.2158203125,
        "lora_inference_memory": 159.8349609375
    },
    {
        "seed": 331522,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 1.4459385871887207,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3635.0224609375,
        "base_inference_memory": 643.3447265625,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 2.4490015506744385,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3653.7255859375,
        "lora_inference_memory": 643.3447265625
    },
    {
        "seed": 533054,
        "category": "high_school_european_history",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 1.206265926361084,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3267.2900390625,
        "base_inference_memory": 275.6123046875,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 1.2362926006317139,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3285.6650390625,
        "lora_inference_memory": 275.2841796875
    },
    {
        "seed": 434667,
        "category": "college_biology",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 1.0341835021972656,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3238.5224609375,
        "base_inference_memory": 246.8447265625,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 1.0680804252624512,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3257.2255859375,
        "lora_inference_memory": 246.8447265625
    },
    {
        "seed": 321691,
        "category": "college_physics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 1.0029242038726807,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3252.6513671875,
        "base_inference_memory": 260.9736328125,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 1.036125659942627,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3270.3779296875,
        "lora_inference_memory": 259.9970703125
    },
    {
        "seed": 303854,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.5407388210296631,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.3125,
        "base_inference_memory": 146.634765625,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.557502269744873,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.015625,
        "lora_inference_memory": 146.634765625
    },
    {
        "seed": 345620,
        "category": "college_biology",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 0.7758138179779053,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3172.7587890625,
        "base_inference_memory": 181.0810546875,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 0.7122499942779541,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3190.8447265625,
        "lora_inference_memory": 180.4638671875
    },
    {
        "seed": 437695,
        "category": "business_ethics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.4773530960083008,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.92333984375,
        "base_inference_memory": 148.24560546875,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 0.49747586250305176,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.62646484375,
        "lora_inference_memory": 148.24560546875
    },
    {
        "seed": 455637,
        "category": "high_school_chemistry",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.6283471584320068,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3172.7587890625,
        "base_inference_memory": 181.0810546875,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.5002596378326416,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3190.8447265625,
        "lora_inference_memory": 180.4638671875
    },
    {
        "seed": 275416,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.5772881507873535,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3226.3818359375,
        "base_inference_memory": 234.7041015625,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.6167206764221191,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3245.0849609375,
        "lora_inference_memory": 234.7041015625
    },
    {
        "seed": 538346,
        "category": [
            "professional_psychology"
        ],
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.603538990020752,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3149.4892578125,
        "base_inference_memory": 157.8115234375,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.6284925937652588,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3168.1923828125,
        "lora_inference_memory": 157.8115234375
    },
    {
        "seed": 122825,
        "category": "college_physics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.5949828624725342,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.576171875,
        "base_inference_memory": 152.8984375,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.5854599475860596,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.279296875,
        "lora_inference_memory": 152.8984375
    },
    {
        "seed": 720661,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.5121724605560303,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3218.2880859375,
        "base_inference_memory": 226.6103515625,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.5492281913757324,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3236.9912109375,
        "lora_inference_memory": 226.6103515625
    },
    {
        "seed": 328829,
        "category": "business_ethics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 1,
        "base_time": 0.3564033508300781,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3132.03759765625,
        "base_inference_memory": 140.35986328125,
        "lora_predict": 0,
        "lora_answer": 1,
        "lora_time": 0.3709084987640381,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3150.74072265625,
        "lora_inference_memory": 140.35986328125
    },
    {
        "seed": 959015,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.2971622943878174,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.60107421875,
        "base_inference_memory": 147.92333984375,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.3231678009033203,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.30419921875,
        "lora_inference_memory": 147.92333984375
    },
    {
        "seed": 629634,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.456707239151001,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3210.9248046875,
        "base_inference_memory": 219.2470703125,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.48650240898132324,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3229.5888671875,
        "lora_inference_memory": 219.2080078125
    },
    {
        "seed": 322581,
        "category": "clinical_knowledge",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.4249117374420166,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3198.3662109375,
        "base_inference_memory": 206.6884765625,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.4525723457336426,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3216.3349609375,
        "lora_inference_memory": 205.9541015625
    },
    {
        "seed": 884703,
        "category": "formal_logic",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.40183424949645996,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.84814453125,
        "base_inference_memory": 152.17041015625,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.4236321449279785,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.55126953125,
        "lora_inference_memory": 152.17041015625
    },
    {
        "seed": 536684,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 1,
        "base_time": 0.4892997741699219,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3256.8603515625,
        "base_inference_memory": 265.1826171875,
        "lora_predict": 0,
        "lora_answer": 1,
        "lora_time": 0.5302994251251221,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3275.4697265625,
        "lora_inference_memory": 265.0888671875
    },
    {
        "seed": 405082,
        "category": "marketing",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.24407196044921875,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3140.08447265625,
        "base_inference_memory": 148.40673828125,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.2639169692993164,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3158.78759765625,
        "lora_inference_memory": 148.40673828125
    },
    {
        "seed": 296693,
        "category": "college_mathematics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 2.5306036472320557,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3601.3505859375,
        "base_inference_memory": 609.6728515625,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 2.5177364349365234,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3619.0615234375,
        "lora_inference_memory": 608.6806640625
    },
    {
        "seed": 397137,
        "category": "college_medicine",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.9633462429046631,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3165.6767578125,
        "base_inference_memory": 173.9990234375,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.6630363464355469,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3183.7080078125,
        "lora_inference_memory": 173.3271484375
    },
    {
        "seed": 971871,
        "category": "high_school_computer_science",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 0,
        "base_time": 0.9982740879058838,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3290.2353515625,
        "base_inference_memory": 298.5576171875,
        "lora_predict": 3,
        "lora_answer": 0,
        "lora_time": 1.0109913349151611,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3308.0947265625,
        "lora_inference_memory": 297.7138671875
    },
    {
        "seed": 911892,
        "category": "prehistory",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 1.247753381729126,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3426.0126953125,
        "base_inference_memory": 434.3349609375,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 1.3078277111053467,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3444.7158203125,
        "lora_inference_memory": 434.3349609375
    },
    {
        "seed": 480568,
        "category": "college_computer_science",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 1,
        "base_time": 0.8069858551025391,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.28173828125,
        "base_inference_memory": 151.60400390625,
        "lora_predict": 0,
        "lora_answer": 1,
        "lora_time": 0.8262028694152832,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3161.98486328125,
        "lora_inference_memory": 151.60400390625
    },
    {
        "seed": 586825,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.5538730621337891,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3183.7626953125,
        "base_inference_memory": 192.0849609375,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.5782792568206787,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3202.0595703125,
        "lora_inference_memory": 191.6787109375
    },
    {
        "seed": 47047,
        "category": "conceptual_physics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.37799692153930664,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.73681640625,
        "base_inference_memory": 144.05908203125,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.39479589462280273,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.43994140625,
        "lora_inference_memory": 144.05908203125
    },
    {
        "seed": 68305,
        "category": "astronomy",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.37433505058288574,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3139.27880859375,
        "base_inference_memory": 147.60107421875,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.40102553367614746,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.98193359375,
        "lora_inference_memory": 147.60107421875
    },
    {
        "seed": 298759,
        "category": "marketing",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 1,
        "base_time": 1.8351342678070068,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3795.5634765625,
        "base_inference_memory": 803.8857421875,
        "lora_predict": 0,
        "lora_answer": 1,
        "lora_time": 1.9427299499511719,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3813.7978515625,
        "lora_inference_memory": 803.4169921875
    },
    {
        "seed": 960116,
        "category": "marketing",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 0.5879220962524414,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3190.0224609375,
        "base_inference_memory": 198.3447265625,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 0.6072945594787598,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3208.1787109375,
        "lora_inference_memory": 197.7978515625
    },
    {
        "seed": 454349,
        "category": "high_school_world_history",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.33597707748413086,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3135.57568359375,
        "base_inference_memory": 143.89794921875,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.3508474826812744,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.27880859375,
        "lora_inference_memory": 143.89794921875
    },
    {
        "seed": 736120,
        "category": "high_school_macroeconomics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 0.5979294776916504,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.41162109375,
        "base_inference_memory": 151.73388671875,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 0.6235387325286865,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3161.84912109375,
        "lora_inference_memory": 151.46826171875
    },
    {
        "seed": 29379,
        "category": "management",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 1.3926470279693604,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3739.1982421875,
        "base_inference_memory": 747.5205078125,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 1.4904711246490479,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3757.7216796875,
        "lora_inference_memory": 747.3408203125
    },
    {
        "seed": 228480,
        "category": "high_school_geography",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 0.5258655548095703,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3146.9931640625,
        "base_inference_memory": 155.3154296875,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 0.7126262187957764,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3165.1572265625,
        "lora_inference_memory": 154.7763671875
    },
    {
        "seed": 74834,
        "category": "high_school_european_history",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.35195112228393555,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.990234375,
        "base_inference_memory": 146.3125,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.35990142822265625,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3156.693359375,
        "lora_inference_memory": 146.3125
    },
    {
        "seed": 602816,
        "category": "us_foreign_policy",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 0,
        "base_time": 1.0232388973236084,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3487.1865234375,
        "base_inference_memory": 495.5087890625,
        "lora_predict": 2,
        "lora_answer": 0,
        "lora_time": 2.135756731033325,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3505.8896484375,
        "lora_inference_memory": 495.5087890625
    },
    {
        "seed": 446056,
        "category": "high_school_geography",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 1.2167792320251465,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3209.8662109375,
        "base_inference_memory": 218.1884765625,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 1.4094336032867432,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3228.5693359375,
        "lora_inference_memory": 218.1884765625
    },
    {
        "seed": 987524,
        "category": "professional_law",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 1.214371681213379,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.994140625,
        "base_inference_memory": 152.31640625,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 0.8648166656494141,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.697265625,
        "lora_inference_memory": 152.31640625
    },
    {
        "seed": 121835,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 2,
        "base_time": 1.6999220848083496,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3624.3388671875,
        "base_inference_memory": 632.6611328125,
        "lora_predict": 3,
        "lora_answer": 2,
        "lora_time": 1.723262071609497,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3642.5107421875,
        "lora_inference_memory": 632.1298828125
    },
    {
        "seed": 351515,
        "category": "college_physics",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 0.7637894153594971,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.34619140625,
        "base_inference_memory": 151.66845703125,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.778083324432373,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.04931640625,
        "lora_inference_memory": 151.66845703125
    },
    {
        "seed": 607704,
        "category": "high_school_us_history",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 0,
        "base_time": 0.7192344665527344,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.28173828125,
        "base_inference_memory": 151.60400390625,
        "lora_predict": 3,
        "lora_answer": 0,
        "lora_time": 0.7564866542816162,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3161.98486328125,
        "lora_inference_memory": 151.60400390625
    },
    {
        "seed": 6171,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 0.39790964126586914,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3138.79541015625,
        "base_inference_memory": 147.11767578125,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 0.38614511489868164,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3157.49853515625,
        "lora_inference_memory": 147.11767578125
    },
    {
        "seed": 144100,
        "category": "college_chemistry",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.3579273223876953,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3137.50732421875,
        "base_inference_memory": 145.82958984375,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.3833296298980713,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3156.21044921875,
        "lora_inference_memory": 145.82958984375
    },
    {
        "seed": 498316,
        "category": "computer_security",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.7283132076263428,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3254.7744140625,
        "base_inference_memory": 263.0966796875,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.7633740901947021,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3273.4306640625,
        "lora_inference_memory": 263.0498046875
    },
    {
        "seed": 327090,
        "category": [
            "professional_psychology"
        ],
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.33369874954223633,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3136.54150390625,
        "base_inference_memory": 144.86376953125,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.3495781421661377,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3155.24462890625,
        "lora_inference_memory": 144.86376953125
    },
    {
        "seed": 288265,
        "category": "high_school_geography",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 3,
        "base_time": 1.1413319110870361,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3547.3408203125,
        "base_inference_memory": 555.6630859375,
        "lora_predict": 0,
        "lora_answer": 3,
        "lora_time": 1.1556875705718994,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3566.0439453125,
        "lora_inference_memory": 555.6630859375
    },
    {
        "seed": 8177,
        "category": "professional_medicine",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 0,
        "base_time": 0.40413403511047363,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3171.8720703125,
        "base_inference_memory": 180.1943359375,
        "lora_predict": 1,
        "lora_answer": 0,
        "lora_time": 0.42695188522338867,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3189.8251953125,
        "lora_inference_memory": 179.4443359375
    },
    {
        "seed": 794299,
        "category": "medical_genetics",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.3923060894012451,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3157.5830078125,
        "base_inference_memory": 165.9052734375,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.40176868438720703,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3176.2861328125,
        "lora_inference_memory": 165.9052734375
    },
    {
        "seed": 30187,
        "category": "high_school_world_history",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 2,
        "base_time": 0.2775444984436035,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3136.05859375,
        "base_inference_memory": 144.380859375,
        "lora_predict": 1,
        "lora_answer": 2,
        "lora_time": 0.29915952682495117,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3154.76171875,
        "lora_inference_memory": 144.380859375
    },
    {
        "seed": 868366,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 1,
        "base_time": 0.44550061225891113,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3151.5126953125,
        "base_inference_memory": 159.8349609375,
        "lora_predict": 2,
        "lora_answer": 1,
        "lora_time": 0.472104549407959,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3170.2158203125,
        "lora_inference_memory": 159.8349609375
    },
    {
        "seed": 918829,
        "category": "college_physics",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.8041877746582031,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3576.3173828125,
        "base_inference_memory": 584.6396484375,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.8864762783050537,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3594.5908203125,
        "lora_inference_memory": 584.2099609375
    },
    {
        "seed": 850284,
        "category": "electrical_engineering",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 1.2749354839324951,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3659.8447265625,
        "base_inference_memory": 668.1669921875,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 3.186460256576538,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3678.1962890625,
        "lora_inference_memory": 667.8154296875
    },
    {
        "seed": 939871,
        "category": "us_foreign_policy",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 1.1891303062438965,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3202.5380859375,
        "base_inference_memory": 210.8603515625,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 1.3239905834197998,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3220.4130859375,
        "lora_inference_memory": 210.0322265625
    },
    {
        "seed": 49029,
        "category": "high_school_us_history",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 3,
        "base_time": 0.8869640827178955,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3186.4150390625,
        "base_inference_memory": 194.7373046875,
        "lora_predict": 3,
        "lora_answer": 3,
        "lora_time": 1.0905475616455078,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3205.1181640625,
        "lora_inference_memory": 194.7373046875
    },
    {
        "seed": 368596,
        "category": "security_studies",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 1,
        "base_time": 1.0166230201721191,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3145.01318359375,
        "base_inference_memory": 153.33544921875,
        "lora_predict": 1,
        "lora_answer": 1,
        "lora_time": 0.9386248588562012,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.71630859375,
        "lora_inference_memory": 153.33544921875
    },
    {
        "seed": 375544,
        "category": "prehistory",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 0,
        "base_time": 0.6003391742706299,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3185.8486328125,
        "base_inference_memory": 194.1708984375,
        "lora_predict": 0,
        "lora_answer": 0,
        "lora_time": 0.6253471374511719,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3204.0986328125,
        "lora_inference_memory": 193.7177734375
    },
    {
        "seed": 732636,
        "category": "college_computer_science",
        "sample_amount": 2,
        "base_predict": 1,
        "base_answer": 3,
        "base_time": 0.812391996383667,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3151.5126953125,
        "base_inference_memory": 159.8349609375,
        "lora_predict": 1,
        "lora_answer": 3,
        "lora_time": 0.8349132537841797,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3170.2158203125,
        "lora_inference_memory": 159.8349609375
    },
    {
        "seed": 42649,
        "category": "high_school_computer_science",
        "sample_amount": 2,
        "base_predict": 0,
        "base_answer": 2,
        "base_time": 0.6817307472229004,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3202.5380859375,
        "base_inference_memory": 210.8603515625,
        "lora_predict": 0,
        "lora_answer": 2,
        "lora_time": 0.7080066204071045,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3220.4130859375,
        "lora_inference_memory": 210.0322265625
    },
    {
        "seed": 963364,
        "category": "college_medicine",
        "sample_amount": 2,
        "base_predict": 3,
        "base_answer": 1,
        "base_time": 0.7274153232574463,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3143.31396484375,
        "base_inference_memory": 151.63623046875,
        "lora_predict": 3,
        "lora_answer": 1,
        "lora_time": 0.7437129020690918,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3162.01708984375,
        "lora_inference_memory": 151.63623046875
    },
    {
        "seed": 222206,
        "category": "logical_fallacies",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 2.048994541168213,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3856.2685546875,
        "base_inference_memory": 864.5908203125,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 2.0619688034057617,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3874.9716796875,
        "lora_inference_memory": 864.5908203125
    },
    {
        "seed": 518127,
        "category": "econometrics",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 3,
        "base_time": 0.6546416282653809,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 3144.86767578125,
        "base_inference_memory": 153.18994140625,
        "lora_predict": 2,
        "lora_answer": 3,
        "lora_time": 0.6692287921905518,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 3163.57080078125,
        "lora_inference_memory": 153.18994140625
    },
    {
        "seed": 294296,
        "category": "public_relations",
        "sample_amount": 2,
        "base_predict": 2,
        "base_answer": 2,
        "base_time": 2.086899518966675,
        "base_initial_memory": 2991.677734375,
        "base_end_memory": 4004.3779296875,
        "base_inference_memory": 1012.7001953125,
        "lora_predict": 2,
        "lora_answer": 2,
        "lora_time": 2.1002962589263916,
        "lora_initial_memory": 3010.380859375,
        "lora_end_memory": 4022.8076171875,
        "lora_inference_memory": 1012.4267578125
    }
]