{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer,AutoModelForSeq2SeqLM,TrainingArguments,Trainer,DataCollatorForLanguageModeling,TextDataset\n",
    "from peft import LoraConfig,TaskType,get_peft_model\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r = 16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model_id = 'openai-community/gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 125,029,632 || trainable%: 0.4717473694555863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:861: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model = get_peft_model(model,peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): lora.Linear(\n",
      "            (base_layer): Conv1D()\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "))\n",
      "('transformer', GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): lora.Linear(\n",
      "          (base_layer): Conv1D()\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "))\n",
      "('transformer.wte', Embedding(50257, 768))\n",
      "('transformer.wpe', Embedding(1024, 768))\n",
      "('transformer.drop', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h', ModuleList(\n",
      "  (0-11): 12 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): lora.Linear(\n",
      "        (base_layer): Conv1D()\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "      (c_proj): Conv1D()\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('transformer.h.0', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.0.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.0.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.0.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.0.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.0.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.0.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.0.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.0.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.0.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.0.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.0.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.0.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.0.attn.c_proj', Conv1D())\n",
      "('transformer.h.0.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.0.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.0.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.0.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.0.mlp.c_fc', Conv1D())\n",
      "('transformer.h.0.mlp.c_proj', Conv1D())\n",
      "('transformer.h.0.mlp.act', NewGELUActivation())\n",
      "('transformer.h.0.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.1.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.1.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.1.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.1.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.1.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.1.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.1.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.1.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.1.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.1.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.1.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.1.attn.c_proj', Conv1D())\n",
      "('transformer.h.1.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.1.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.1.mlp.c_fc', Conv1D())\n",
      "('transformer.h.1.mlp.c_proj', Conv1D())\n",
      "('transformer.h.1.mlp.act', NewGELUActivation())\n",
      "('transformer.h.1.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.2.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.2.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.2.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.2.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.2.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.2.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.2.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.2.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.2.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.2.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.2.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.2.attn.c_proj', Conv1D())\n",
      "('transformer.h.2.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.2.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.2.mlp.c_fc', Conv1D())\n",
      "('transformer.h.2.mlp.c_proj', Conv1D())\n",
      "('transformer.h.2.mlp.act', NewGELUActivation())\n",
      "('transformer.h.2.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.3.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.3.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.3.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.3.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.3.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.3.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.3.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.3.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.3.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.3.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.3.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.3.attn.c_proj', Conv1D())\n",
      "('transformer.h.3.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.3.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.3.mlp.c_fc', Conv1D())\n",
      "('transformer.h.3.mlp.c_proj', Conv1D())\n",
      "('transformer.h.3.mlp.act', NewGELUActivation())\n",
      "('transformer.h.3.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.4.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.4.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.4.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.4.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.4.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.4.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.4.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.4.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.4.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.4.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.4.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.4.attn.c_proj', Conv1D())\n",
      "('transformer.h.4.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.4.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.4.mlp.c_fc', Conv1D())\n",
      "('transformer.h.4.mlp.c_proj', Conv1D())\n",
      "('transformer.h.4.mlp.act', NewGELUActivation())\n",
      "('transformer.h.4.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.5.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.5.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.5.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.5.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.5.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.5.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.5.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.5.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.5.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.5.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.5.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.5.attn.c_proj', Conv1D())\n",
      "('transformer.h.5.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.5.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.5.mlp.c_fc', Conv1D())\n",
      "('transformer.h.5.mlp.c_proj', Conv1D())\n",
      "('transformer.h.5.mlp.act', NewGELUActivation())\n",
      "('transformer.h.5.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.6.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.6.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.6.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.6.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.6.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.6.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.6.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.6.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.6.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.6.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.6.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.6.attn.c_proj', Conv1D())\n",
      "('transformer.h.6.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.6.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.6.mlp.c_fc', Conv1D())\n",
      "('transformer.h.6.mlp.c_proj', Conv1D())\n",
      "('transformer.h.6.mlp.act', NewGELUActivation())\n",
      "('transformer.h.6.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.7.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.7.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.7.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.7.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.7.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.7.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.7.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.7.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.7.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.7.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.7.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.7.attn.c_proj', Conv1D())\n",
      "('transformer.h.7.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.7.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.7.mlp.c_fc', Conv1D())\n",
      "('transformer.h.7.mlp.c_proj', Conv1D())\n",
      "('transformer.h.7.mlp.act', NewGELUActivation())\n",
      "('transformer.h.7.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.8.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.8.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.8.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.8.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.8.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.8.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.8.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.8.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.8.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.8.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.8.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.8.attn.c_proj', Conv1D())\n",
      "('transformer.h.8.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.8.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.8.mlp.c_fc', Conv1D())\n",
      "('transformer.h.8.mlp.c_proj', Conv1D())\n",
      "('transformer.h.8.mlp.act', NewGELUActivation())\n",
      "('transformer.h.8.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.9.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.9.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.9.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.9.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.9.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.9.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.9.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.9.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.9.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.9.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.9.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.9.attn.c_proj', Conv1D())\n",
      "('transformer.h.9.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.9.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.9.mlp.c_fc', Conv1D())\n",
      "('transformer.h.9.mlp.c_proj', Conv1D())\n",
      "('transformer.h.9.mlp.act', NewGELUActivation())\n",
      "('transformer.h.9.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.10.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.10.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.10.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.10.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.10.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.10.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.10.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.10.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.10.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.10.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.10.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.10.attn.c_proj', Conv1D())\n",
      "('transformer.h.10.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.10.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.10.mlp.c_fc', Conv1D())\n",
      "('transformer.h.10.mlp.c_proj', Conv1D())\n",
      "('transformer.h.10.mlp.act', NewGELUActivation())\n",
      "('transformer.h.10.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): lora.Linear(\n",
      "      (base_layer): Conv1D()\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "    )\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.11.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.11.attn', GPT2Attention(\n",
      "  (c_attn): lora.Linear(\n",
      "    (base_layer): Conv1D()\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.11.attn.c_attn', lora.Linear(\n",
      "  (base_layer): Conv1D()\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "))\n",
      "('transformer.h.11.attn.c_attn.base_layer', Conv1D())\n",
      "('transformer.h.11.attn.c_attn.lora_dropout', ModuleDict(\n",
      "  (default): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.11.attn.c_attn.lora_dropout.default', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11.attn.c_attn.lora_A', ModuleDict(\n",
      "  (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "))\n",
      "('transformer.h.11.attn.c_attn.lora_A.default', Linear(in_features=768, out_features=16, bias=False))\n",
      "('transformer.h.11.attn.c_attn.lora_B', ModuleDict(\n",
      "  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
      "))\n",
      "('transformer.h.11.attn.c_attn.lora_B.default', Linear(in_features=16, out_features=2304, bias=False))\n",
      "('transformer.h.11.attn.c_attn.lora_embedding_A', ParameterDict())\n",
      "('transformer.h.11.attn.c_attn.lora_embedding_B', ParameterDict())\n",
      "('transformer.h.11.attn.c_proj', Conv1D())\n",
      "('transformer.h.11.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.11.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.11.mlp.c_fc', Conv1D())\n",
      "('transformer.h.11.mlp.c_proj', Conv1D())\n",
      "('transformer.h.11.mlp.act', NewGELUActivation())\n",
      "('transformer.h.11.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.ln_f', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('lm_head', Linear(in_features=768, out_features=50257, bias=False))\n"
     ]
    }
   ],
   "source": [
    "for layer in model.get_base_model().named_modules():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments (\n",
    "    output_dir='output/test',\n",
    "    learning_rate=1e-3,\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=16,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='no',\n",
    "    save_strategy='epoch',\n",
    "    #load_best_model_at_end='True',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_path = 'intents.json'):\n",
    "    with open(input_path,'r') as file:\n",
    "        data = json.load(file)\n",
    "    preprocess_data = []\n",
    "    for intent in data['intents']:\n",
    "        for patternn in intent['patterns']:\n",
    "            preprocess_data.append(f'User: {patternn}\\n')\n",
    "            for response in intent['responses']:\n",
    "                preprocess_data.append(f'Assistant: {response}\\n')\n",
    "    return ''.join(preprocess_data)\n",
    "def save_preprocess(data,output_path = 'data/data.txt'):\n",
    "    with open(output_path,'w') as file:\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess()\n",
    "save_preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='data/data.txt',\n",
    "    block_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=  training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6a75235b604808a42f14d00511859c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 81.8465, 'train_samples_per_second': 49.849, 'train_steps_per_second': 1.564, 'train_loss': 2.0827982425689697, 'epoch': 16.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=128, training_loss=2.0827982425689697, metrics={'train_runtime': 81.8465, 'train_samples_per_second': 49.849, 'train_steps_per_second': 1.564, 'train_loss': 2.0827982425689697, 'epoch': 16.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"output_dir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
