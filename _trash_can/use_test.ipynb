{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer,AutoModelForSeq2SeqLM,TrainingArguments,Trainer,DataCollatorForLanguageModeling,TextDataset\n",
    "from peft import LoraConfig,TaskType,get_peft_model\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "D:\\Programming\\Storage\\Class\\Project_F\\output_dir does not appear to have a file named config.json. Checkout 'https://huggingface.co/D:\\Programming\\Storage\\Class\\Project_F\\output_dir/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProgramming\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mStorage\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject_F\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:461\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    459\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 461\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    462\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    463\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    464\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:983\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    982\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 983\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    984\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    985\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 617\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    619\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 672\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:388\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[1;32m--> 388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    389\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    390\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         )\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: D:\\Programming\\Storage\\Class\\Project_F\\output_dir does not appear to have a file named config.json. Checkout 'https://huggingface.co/D:\\Programming\\Storage\\Class\\Project_F\\output_dir/None' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "path = 'D:\\Programming\\Storage\\Class\\Project_F\\output_dir'\n",
    "model = AutoModelForCausalLM.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel,LoraConfig,TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "\n",
    "model_name = 'openai-community/gpt2'\n",
    "adapters_name = \"output_dir\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model openai-community/gpt2 into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:861: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 36,864 || all params: 124,476,672 || trainable%: 0.029615187655402612\n",
      "trainable params: 0 || all params: 125,619,456 || trainable%: 0.0\n",
      "Successfully loaded the model openai-community/gpt2 into memory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #load_in_4bit=True,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    #device_map={\"\": 0}\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r = 1,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = PeftModel(model,peft_config=peft_config)\n",
    "model.print_trainable_parameters()\n",
    "m = PeftModel.from_pretrained(model, adapters_name,local_files_only = True)\n",
    "#m = m.merge_and_unload()\n",
    "m.print_trainable_parameters()\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "tok.bos_token_id = 1\n",
    "\n",
    "stop_token_ids = [0]\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text,max_length = 256):\n",
    "    start = time.time()\n",
    "    output = model.base_model().generate(input_text,max_length=max_length)\n",
    "    print('\\n Ouput : \\n',output)\n",
    "    end = time.time()\n",
    "    print(f'Total time elapsed : {end-start:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,return_dict_in_generate\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,output_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m generated_tokens_ids \u001b[38;5;241m=\u001b[39m model_outputs\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_tokens_ids)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_outputs = model.generate(**inputs,max_new_tokens = 32,return_dict_in_generate= True,output_scores=True)\n",
    "generated_tokens_ids = model_outputs.sequences[0]\n",
    "\n",
    "tokenizer.decode(generated_tokens_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Who are you ? Can you believe all these crazy guys I saw here?\"\\n\\nTyson, who had been attending the event with a friend, walked up to him. \"You two have some crazy issues,\" he said, \"can you believe'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation',model=model_name)\n",
    "generator('Who are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 36,864 || all params: 124,476,672 || trainable%: 0.029615187655402612\n",
      "[8241  389  345 5633  198  198   40  716  257  582  286 1793   13  198\n",
      "  198   40  716  257  582  286 1793   13  198  198   40  716  257  582\n",
      "  286 1793   13  198  198   40  716  257]\n",
      "Who are you?\n",
      "\n",
      "I am a man of God.\n",
      "\n",
      "I am a man of God.\n",
      "\n",
      "I am a man of God.\n",
      "\n",
      "I am a\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "x = \"Who are you ?\"\n",
    "inputs = tokenizer(x, return_tensors=\"pt\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'model',\n",
    "    #load_in_4bit=True,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    #device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r = 1,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "perft_config = {\n",
    "    'max_tokens': 1024\n",
    "}\n",
    "model = PeftModel(model,peft_config=peft_config)\n",
    "#model.load_adapter('output_dir',adapters_name)\n",
    "#model.set_adapter(adapters_name)\n",
    "#model.unload()\n",
    "#model.merge_adapter()\n",
    "model.print_trainable_parameters()\n",
    "model_outputs = model.generate(**inputs,max_new_tokens = 32)\n",
    "generated_tokens_ids = model_outputs.numpy()[0]\n",
    "print(generated_tokens_ids)\n",
    "result = tokenizer.decode(generated_tokens_ids)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning.\n",
      "Assistant: Hello there. Glad to see you're back. How are you feeling today?\n",
      "Assistant: Hello there. Glad to see you're back\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning, my dear, my dear, my dear, my dear, my dear, my dear, my dear, my dear, my dear, my dear, my\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[15496    11   314  1101  7926    11   475   314  1101   407  1654   611\n",
    "   345   821  3910   286   428    13   314  1101   407  1654   611   345\n",
    "   821  3910   286   428    13   314  1101   407  1654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "))\n",
      "('transformer', GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "))\n",
      "('transformer.wte', Embedding(50257, 768))\n",
      "('transformer.wpe', Embedding(1024, 768))\n",
      "('transformer.drop', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h', ModuleList(\n",
      "  (0-11): 12 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('transformer.h.0', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.0.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.0.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.0.attn.c_attn', Conv1D())\n",
      "('transformer.h.0.attn.c_proj', Conv1D())\n",
      "('transformer.h.0.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.0.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.0.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.0.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.0.mlp.c_fc', Conv1D())\n",
      "('transformer.h.0.mlp.c_proj', Conv1D())\n",
      "('transformer.h.0.mlp.act', NewGELUActivation())\n",
      "('transformer.h.0.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.1.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.1.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.1.attn.c_attn', Conv1D())\n",
      "('transformer.h.1.attn.c_proj', Conv1D())\n",
      "('transformer.h.1.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.1.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.1.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.1.mlp.c_fc', Conv1D())\n",
      "('transformer.h.1.mlp.c_proj', Conv1D())\n",
      "('transformer.h.1.mlp.act', NewGELUActivation())\n",
      "('transformer.h.1.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.2.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.2.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.2.attn.c_attn', Conv1D())\n",
      "('transformer.h.2.attn.c_proj', Conv1D())\n",
      "('transformer.h.2.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.2.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.2.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.2.mlp.c_fc', Conv1D())\n",
      "('transformer.h.2.mlp.c_proj', Conv1D())\n",
      "('transformer.h.2.mlp.act', NewGELUActivation())\n",
      "('transformer.h.2.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.3.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.3.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.3.attn.c_attn', Conv1D())\n",
      "('transformer.h.3.attn.c_proj', Conv1D())\n",
      "('transformer.h.3.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.3.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.3.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.3.mlp.c_fc', Conv1D())\n",
      "('transformer.h.3.mlp.c_proj', Conv1D())\n",
      "('transformer.h.3.mlp.act', NewGELUActivation())\n",
      "('transformer.h.3.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.4.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.4.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.4.attn.c_attn', Conv1D())\n",
      "('transformer.h.4.attn.c_proj', Conv1D())\n",
      "('transformer.h.4.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.4.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.4.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.4.mlp.c_fc', Conv1D())\n",
      "('transformer.h.4.mlp.c_proj', Conv1D())\n",
      "('transformer.h.4.mlp.act', NewGELUActivation())\n",
      "('transformer.h.4.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.5.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.5.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.5.attn.c_attn', Conv1D())\n",
      "('transformer.h.5.attn.c_proj', Conv1D())\n",
      "('transformer.h.5.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.5.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.5.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.5.mlp.c_fc', Conv1D())\n",
      "('transformer.h.5.mlp.c_proj', Conv1D())\n",
      "('transformer.h.5.mlp.act', NewGELUActivation())\n",
      "('transformer.h.5.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.6.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.6.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.6.attn.c_attn', Conv1D())\n",
      "('transformer.h.6.attn.c_proj', Conv1D())\n",
      "('transformer.h.6.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.6.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.6.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.6.mlp.c_fc', Conv1D())\n",
      "('transformer.h.6.mlp.c_proj', Conv1D())\n",
      "('transformer.h.6.mlp.act', NewGELUActivation())\n",
      "('transformer.h.6.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.7.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.7.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.7.attn.c_attn', Conv1D())\n",
      "('transformer.h.7.attn.c_proj', Conv1D())\n",
      "('transformer.h.7.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.7.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.7.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.7.mlp.c_fc', Conv1D())\n",
      "('transformer.h.7.mlp.c_proj', Conv1D())\n",
      "('transformer.h.7.mlp.act', NewGELUActivation())\n",
      "('transformer.h.7.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.8.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.8.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.8.attn.c_attn', Conv1D())\n",
      "('transformer.h.8.attn.c_proj', Conv1D())\n",
      "('transformer.h.8.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.8.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.8.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.8.mlp.c_fc', Conv1D())\n",
      "('transformer.h.8.mlp.c_proj', Conv1D())\n",
      "('transformer.h.8.mlp.act', NewGELUActivation())\n",
      "('transformer.h.8.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.9.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.9.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.9.attn.c_attn', Conv1D())\n",
      "('transformer.h.9.attn.c_proj', Conv1D())\n",
      "('transformer.h.9.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.9.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.9.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.9.mlp.c_fc', Conv1D())\n",
      "('transformer.h.9.mlp.c_proj', Conv1D())\n",
      "('transformer.h.9.mlp.act', NewGELUActivation())\n",
      "('transformer.h.9.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.10.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.10.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.10.attn.c_attn', Conv1D())\n",
      "('transformer.h.10.attn.c_proj', Conv1D())\n",
      "('transformer.h.10.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.10.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.10.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.10.mlp.c_fc', Conv1D())\n",
      "('transformer.h.10.mlp.c_proj', Conv1D())\n",
      "('transformer.h.10.mlp.act', NewGELUActivation())\n",
      "('transformer.h.10.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11', GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "))\n",
      "('transformer.h.11.ln_1', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.11.attn', GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.11.attn.c_attn', Conv1D())\n",
      "('transformer.h.11.attn.c_proj', Conv1D())\n",
      "('transformer.h.11.attn.attn_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11.attn.resid_dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.h.11.ln_2', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('transformer.h.11.mlp', GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "))\n",
      "('transformer.h.11.mlp.c_fc', Conv1D())\n",
      "('transformer.h.11.mlp.c_proj', Conv1D())\n",
      "('transformer.h.11.mlp.act', NewGELUActivation())\n",
      "('transformer.h.11.mlp.dropout', Dropout(p=0.1, inplace=False))\n",
      "('transformer.ln_f', LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
      "('lm_head', Linear(in_features=768, out_features=50257, bias=False))\n"
     ]
    }
   ],
   "source": [
    "for layer in model.named_modules():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8241  389  345 5633  198  198   40  716  257  582  286 1793   13  198\n",
      "  198   40  716  257  582  286 1793   13  198  198   40  716  257  582\n",
      "  286 1793   13  198  198   40  716  257]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Who are you?\\n\\nI am a man of God.\\n\\nI am a man of God.\\n\\nI am a man of God.\\n\\nI am a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs = m.generate(**inputs,max_new_tokens = 32)\n",
    "generated_tokens_ids = model_outputs.numpy()[0]\n",
    "print(generated_tokens_ids)\n",
    "tokenizer.decode(generated_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "x = \"Hello world, ...\"\n",
    "inputs = tokenizer(x, return_tensors=\"pt\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "x = \"Hello \"\n",
    "inputs = tokenizer(x, return_tensors=\"pt\")\n",
    "\n",
    "model_outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "generated_tokens_ids = model_outputs.sequences[0]\n",
    "\n",
    "tokenizer.decode(generated_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(m,tokenizer.encode('Hello'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
